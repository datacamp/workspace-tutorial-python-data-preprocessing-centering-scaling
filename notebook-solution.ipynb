{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff0fca4-03a0-4dfa-bd1b-eaa6766e217b",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Machine Learning: Centering and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5d24d-05a7-4f54-a45f-62205ff77e3d",
   "metadata": {},
   "source": [
    "## What is data preprocesing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723436d6-4b37-4a21-9360-d3cf365588fd",
   "metadata": {},
   "source": [
    "**Data preprocessing** means using manipulation techniques to make your dataset ready for running a machine learning model. In particular, you'll be transforming the features (columns containing inputs to the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a1202-e50c-480c-9ddc-0220f254106c",
   "metadata": {},
   "source": [
    "## What are centering and scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76692894-7ac8-447d-9675-bf152f4c0b1b",
   "metadata": {},
   "source": [
    "**Centering** means subtracting the mean of a feature from each element of the feature, so that the mean of the processed feature is zero.\n",
    "\n",
    "**Scaling** means dividing the each element of the feature by the standard deviation, so that the standard deviation of the processed feature is one. Some people prefer the term **standardizing**. It means the same thing as scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dab5ad-6bb3-4ec1-834c-8a6560a4e1c9",
   "metadata": {},
   "source": [
    "## When should I use centering and scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3901bd-fe22-4522-920f-b2d0e8687ffb",
   "metadata": {},
   "source": [
    "Centering and scaling is essential for model types that assume each feature comes from a standard normal distribution. That includes\n",
    "\n",
    "- K-nearest neighbors\n",
    "- Support vector machines (when using the 'kernel trick' of the radial basis function)\n",
    "- Regularized regression (lasso and ridge regression)\n",
    "\n",
    "Centering and scaling are not essential but can help with convergence for the following model types.\n",
    "\n",
    "- Linear and logistic regression\n",
    "- Neural networks\n",
    "\n",
    "Centering and scaling have no effect and are completely unnecessary for the following model types.\n",
    "\n",
    "- Tree-based models (decision trees, random forests, gradient boosting)\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dda652-48f4-4cb1-b8ea-cb5bc2c8c6a4",
   "metadata": {},
   "source": [
    "## What Python packages can I use for centering and scaling data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae56d82-7546-4322-97e3-875558e32552",
   "metadata": {},
   "source": [
    "- **scikit-learn** (used here)\n",
    "- **PyCaret**\n",
    "- **pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a11e8-b881-4b85-a93f-a905b20e29b5",
   "metadata": {},
   "source": [
    "## Case study: k-nearest neighbors using the diamonds dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b00fa-4455-4652-9336-b708e4626d33",
   "metadata": {},
   "source": [
    "The diamonds dataset is a classic dataset on diamond prices, originally found in R's **ggplot2** package, and available to Python users in the **plotnine** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b32af-e96a-4962-a2a2-0a4f1b455039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine.data import diamonds\n",
    "diamonds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa32b33-c674-44e2-b15a-e6862af173fd",
   "metadata": {},
   "source": [
    "We'll try to predict the **cut** of the diamonds using the numeric features in the dataset. (That is, for simplicity, we'll ignore **color** and **clarity**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfafac-4893-43a6-a4fe-c93adb94180a",
   "metadata": {},
   "source": [
    "Before modeling, let's look at some summary statistics in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819363b6-1428-404e-8c04-e749878db6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a137f8-c65e-4260-be72-f7b66bf3bbf8",
   "metadata": {},
   "source": [
    "Notice that the maximum value of carat is about 5, but the maximum price is almost 20000. Sadly, you can't get a one carat diamond for a dollar, so the scales of each feature are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d3f45-23da-4cc2-945f-9c807834701f",
   "metadata": {},
   "source": [
    "## Importing the required functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0f2b3-586d-44db-a49d-9c504a62ce73",
   "metadata": {},
   "source": [
    "We'll use \n",
    "\n",
    "- [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into training and testing sets.\n",
    "- [KNeighborsClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) to fit the k-nearest neighbors model.\n",
    "- [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to scale the features used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f980bf-ed78-43dd-9e36-d9b09305008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44572c2d-0639-4620-992c-1704e54a08b0",
   "metadata": {},
   "source": [
    "## Creating training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc44c5-8588-4bd4-9257-fe4be9886c5c",
   "metadata": {},
   "source": [
    "- The **cut** column is our response variable (the thing to predict). We'll assign this to `y`.\n",
    "- We'll use all the other numeric variables (everything except the response, **color**, and **clarity**) for features. We'll assign these to `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3182e11-209c-4dd8-bbeb-324c47aed685",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diamonds[\"cut\"]\n",
    "X = diamonds.drop(columns=[\"cut\", \"color\", \"clarity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4d599-01b4-4a2b-a769-14254669408b",
   "metadata": {},
   "source": [
    "Now we perform the train-test split, using default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e73ae-9121-43e0-b850-2ed89d10c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65eb594-027a-4611-ab64-6478e7d65418",
   "metadata": {},
   "source": [
    "## Creating a K-NN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39216dce-298b-482d-bcde-e2e624e51c62",
   "metadata": {},
   "source": [
    "To run the k-nearest neighbors model, we need to create a `KNeighborsClassifier` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3bba2-eebc-489d-98bb-554b26a40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ef408-7f6c-4e17-96d7-c6d4e2ecea2b",
   "metadata": {},
   "source": [
    "## Running the model without standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed735e-0ace-46a4-8b39-e1ebd0387aa6",
   "metadata": {},
   "source": [
    "First we fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04d665-db36-43b3-b514-b8b0dd284d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b746b-de30-4712-98ba-fe77c3ad3dc7",
   "metadata": {},
   "source": [
    "Now we measure the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb18ec-a140-4581-93b8-1b454b99b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67311886-1776-40d7-8ed4-ed7143cf58c5",
   "metadata": {},
   "source": [
    "## Running the model with standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b75f95-bcd9-43aa-91e6-d84d4fb5b935",
   "metadata": {},
   "source": [
    "First we create a standard scaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4ea9f-495a-4ef8-b40a-c46b212989ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ee991-ccfd-462f-91e8-0a15c2359ab4",
   "metadata": {},
   "source": [
    "Now we fit the scaler (calculate the means and standard deviations) and transform the features (subtract those means and divide by the standard deviations).\n",
    "\n",
    "It's important that we perform this separately on the training and testing sets. Otherwise we suffer **data leakage**, where information from the testing set has \"leaked\" into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1a702-ff07-4b06-a9d0-1a9923ed4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1cb5a-4d90-4fcb-8177-692557f065e3",
   "metadata": {},
   "source": [
    "Again we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e28bd1-9efe-4cfc-b719-0e7b9dc9a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00654b-735e-4bd9-ab49-69243153606a",
   "metadata": {},
   "source": [
    "... and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d7944-c9bb-48e3-b5a6-65e50834900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e088b-2b88-4f33-a2df-10176b2f543a",
   "metadata": {},
   "source": [
    "Notice the substantial improvement in accuracy. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df36861-cba7-4f02-927b-cb9808cb8e28",
   "metadata": {},
   "source": [
    "## What other types of scaling are available?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa6eb5-ccb6-4d40-98b8-dbaaa4b976f5",
   "metadata": {},
   "source": [
    "Scikit-learn provides several other functions for scaling in the `sklearn.preprocessing` submodule.\n",
    "\n",
    "- [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) subtracts the median and divides by the inter-quartile range.\n",
    "- [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) divides each value by the maximum absolute value (so all values are between -1 and 1).\n",
    "- [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) converts the values to a range.\n",
    "- [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) scales each row so the sum of the squares of the values equals one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598b0f3-0ada-4f86-a277-e9040136dbd8",
   "metadata": {},
   "source": [
    "## Where can I learn more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bc75b-cebb-4770-8a7c-a8571712bd96",
   "metadata": {},
   "source": [
    "- DataCamp's [Preprocessing for Machine Learning in Python](https://app.datacamp.com/learn/courses/preprocessing-for-machine-learning-in-python) and [Feature Engineering for Machine Learning in Python](https://app.datacamp.com/learn/courses/feature-engineering-for-machine-learning-in-python) courses.\n",
    "- scikit-learn's [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html) tutorial.\n",
    "- Quora Q&A on [Which machine algorithms require data scaling/normalization?](https://www.quora.com/Which-machine-algorithms-require-data-scaling-normalization)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
